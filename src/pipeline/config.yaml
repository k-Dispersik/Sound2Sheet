# Sound2Sheet Training Pipeline Configuration

# Dataset Generation
dataset:
  # If you want to use an existing dataset instead of generating a new one:
  # Set use_existing_dataset: true and provide existing_dataset_path
  use_existing_dataset: false  # Set to true to skip dataset generation
  existing_dataset_path: null  # Path to existing dataset
  
  samples: 5000  # Total samples to generate
  complexity: "medium"  # simple, medium, complex (maps to beginner, intermediate, advanced)
  min_notes: 10
  max_notes: 100
  min_duration: 5.0  # seconds
  max_duration: 60.0
  key_signatures: null  # null for all 24 major/minor keys
  time_signatures: null  # null for all time signatures
  output_dir: "data/datasets/training_run"
  synthesize_audio: true  # Always synthesize audio for training
  soundfont_path: null  # Auto-detect if null

# Audio Processing
audio:
  sample_rate: 16000
  n_fft: 1024
  hop_length: 320
  n_mels: 128
  f_min: 0.0
  f_max: 8000.0
  normalize: true
  pre_emphasis: 0.97
  augmentation:
    enabled: true
    noise_type: "random"  # Specific type OR 'random' to randomly choose
    noise_types_pool: ["white", "pink", "ambient"]  # Types to randomly choose from (if noise_type='random')
    noise_level: 0.005
    pitch_shift_range: [-2, 2]  # semitones
    time_stretch_range: [0.9, 1.1]

# Model Architecture
model:
  encoder_name: "MIT/ast-finetuned-audioset-10-10-0.4593"
  freeze_encoder: true  # Start with frozen encoder for faster initial training
  hidden_size: 768
  num_decoder_layers: 6
  num_attention_heads: 8
  dropout: 0.1
  max_sequence_length: 1024  # Increased for longer sequences
  vocab_size: 388  # MIDI notes + special tokens
  device: "cuda"  # cuda, cpu, or auto

# Training Configuration
training:
  # Resume training from checkpoint (optional)
  # Set to checkpoint path to continue training from that point
  # Example: "results/2025-11-04/production_5k_frozen/checkpoints/checkpoint_epoch_50.pt"
  resume_checkpoint: null
  
  batch_size: 8  # Reduced from 16 for 6GB VRAM (RTX 4050)
  num_epochs: 100  # More epochs for serious training
  learning_rate: 0.0001
  weight_decay: 0.01
  warmup_steps: 1000  # Longer warmup for stability
  max_grad_norm: 1.0
  use_mixed_precision: true
  gradient_accumulation_steps: 8  # Effective batch size = 8 * 8 = 64
  num_workers: 4
  # Optionally cap the fraction of GPU memory used by this process (0.0-1.0)
  # 0.45 = ~45% to leave buffer for memory fragmentation
  gpu_memory_fraction: 0.90
  
  # Learning rate schedule
  lr_schedule: "cosine"  # linear, cosine, constant
  
  # Early stopping
  early_stopping_patience: 15  # More patience for better convergence
  early_stopping_min_delta: 0.0005  # More sensitive threshold
  
  # Checkpointing (save only best and final models to save disk space)
  checkpoint_dir: "models/checkpoints"
  save_every_n_epochs: 0  # 0 = don't save intermediate checkpoints, only best + final
  keep_last_n_checkpoints: 2  # Keep only best_model.pt and final_model.pt
  
  # Logging
  log_dir: "logs"
  log_every_n_steps: 50  # Less frequent logging for speed
  eval_every_n_epochs: 1

# Inference Configuration
inference:
  strategy: "beam_search"  # greedy, beam_search, sampling
  max_length: 1024  # Match model's max_sequence_length
  num_beams: 5
  temperature: 1.0
  top_k: 50
  top_p: 0.95
  repetition_penalty: 1.2  # Prevent repeating same token

# Evaluation Configuration
evaluation:
  enabled: true  # Run evaluation after training
  onset_tolerance: 0.05  # seconds
  offset_tolerance: 0.05
  pitch_tolerance: 0  # semitones
  min_duration: 0.1
  max_duration: null
  
  # Visualization
  visualizations:
    enabled: true
    output_dir: "results/visualizations"
    plot_types: ["dashboard", "confusion_matrix", "metrics_over_time"]
  
  # Reports
  reports:
    enabled: true
    output_dir: "results/reports"
    formats: ["json", "csv"]

# Output paths
output:
  model_dir: "models/trained"
  results_dir: "results"
  final_model_name: "sound2sheet_model.pt"

# Sound2Sheet Training Pipeline Configuration

# Dataset Generation
dataset:
  samples: 1000  # Total samples to generate
  complexity: "medium"  # simple, medium, complex (maps to beginner, intermediate, advanced)
  min_notes: 10
  max_notes: 50
  min_duration: 5.0  # seconds
  max_duration: 30.0
  key_signatures: ["C", "G", "D", "F", "Bb"]  # null for all 24
  time_signatures: ["4/4", "3/4", "6/8"]  # null for all
  output_dir: "data/datasets/training_run"
  synthesize_audio: true  # Always synthesize audio for training
  soundfont_path: null  # Auto-detect if null

# Audio Processing
audio:
  sample_rate: 16000
  n_fft: 1024
  hop_length: 320
  n_mels: 128
  f_min: 0.0
  f_max: 8000.0
  normalize: true
  pre_emphasis: 0.97
  augmentation:
    enabled: true
    pitch_shift_range: [-2, 2]  # semitones
    time_stretch_range: [0.9, 1.1]
    noise_level: 0.005

# Model Architecture
model:
  encoder_name: "MIT/ast-finetuned-audioset-10-10-0.4593"
  freeze_encoder: true
  hidden_size: 768
  num_decoder_layers: 6
  num_attention_heads: 8
  dropout: 0.1
  max_sequence_length: 512
  vocab_size: 388  # MIDI notes + special tokens
  device: "cuda"  # cuda, cpu, or auto

# Training Configuration
training:
  batch_size: 16
  num_epochs: 50
  learning_rate: 0.0001
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
  use_mixed_precision: true
  gradient_accumulation_steps: 1
  num_workers: 4
  
  # Learning rate schedule
  lr_schedule: "cosine"  # linear, cosine, constant
  
  # Early stopping
  early_stopping_patience: 10
  early_stopping_min_delta: 0.001
  
  # Checkpointing
  checkpoint_dir: "models/checkpoints"
  save_every_n_epochs: 5
  keep_last_n_checkpoints: 3
  
  # Logging
  log_dir: "logs"
  log_every_n_steps: 10
  eval_every_n_epochs: 1

# Inference Configuration
inference:
  strategy: "beam_search"  # greedy, beam_search, sampling
  max_length: 512
  num_beams: 5
  temperature: 1.0
  top_k: 50
  top_p: 0.95
  repetition_penalty: 1.0

# Evaluation Configuration
evaluation:
  enabled: true  # Run evaluation after training
  onset_tolerance: 0.05  # seconds
  offset_tolerance: 0.05
  pitch_tolerance: 0  # semitones
  min_duration: 0.1
  max_duration: null
  
  # Visualization
  visualizations:
    enabled: true
    output_dir: "results/visualizations"
    plot_types: ["dashboard", "confusion_matrix", "metrics_over_time"]
  
  # Reports
  reports:
    enabled: true
    output_dir: "results/reports"
    formats: ["json", "csv"]

# Output paths
output:
  model_dir: "models/trained"
  results_dir: "results"
  final_model_name: "sound2sheet_model.pt"

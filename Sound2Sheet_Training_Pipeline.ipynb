{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c64483",
   "metadata": {},
   "source": [
    "# Sound2Sheet Training Pipeline\n",
    "\n",
    "Complete training pipeline for audio-to-sheet-music transcription model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a30caa",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7593ecc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2549346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/k-Dispersik/Sound2Sheet.git\n",
    "%cd Sound2Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install system dependencies\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq fluidsynth fluid-soundfont-gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python packages\n",
    "!pip install -q torch torchaudio transformers librosa numpy pandas tqdm pyyaml \\\n",
    "    pretty_midi music21 matplotlib mido midi2audio accelerate scipy scikit-learn\n",
    "\n",
    "print(\"Installation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba8cc902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volodymyr/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Verify imports\n",
    "from src.dataset import DatasetGenerator, MIDIGenerator\n",
    "from src.core import AudioProcessor\n",
    "from src.model import Sound2SheetModel, Trainer, create_dataloaders\n",
    "from src.converter import NoteBuilder\n",
    "from src.evaluation import Evaluator\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a6f019",
   "metadata": {},
   "source": [
    "## Step 2: Generate Dataset\n",
    "\n",
    "Configure and generate synthetic training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9171202c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Samples: 1000\n",
      "  Complexity: beginner:0.3,intermediate:0.5,advanced:0.2\n",
      "  Epochs: 50\n",
      "  Batch size: 16\n",
      "  Learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Dataset configuration\n",
    "SAMPLES = 1000          # Total samples (train + val + test)\n",
    "COMPLEXITY_DIST = \"beginner:0.3,intermediate:0.5,advanced:0.2\"  # Complexity distribution\n",
    "EPOCHS = 50             # Training epochs\n",
    "BATCH_SIZE = 16         # Batch size\n",
    "LEARNING_RATE = 1e-4    # Learning rate\n",
    "EXPERIMENT_NAME = \"training_run_1\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Samples: {SAMPLES}\")\n",
    "print(f\"  Complexity: {COMPLEXITY_DIST}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b9eb523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset generator...\n",
      "2025-11-06 20:50:52,412 - src.dataset.dataset_generator - INFO - Generating dataset: training_run_1 v1.0.0\n",
      "2025-11-06 20:50:52,412 - src.dataset.dataset_generator - INFO - Total samples: 1000\n",
      "2025-11-06 20:50:52,414 - src.dataset.dataset_generator - INFO - Split: train=700, val=150, test=150\n",
      "2025-11-06 20:53:06,681 - src.dataset.dataset_generator - INFO - ✓ train: 700 samples, 488.6 MB audio + 378.3 KB MIDI\n",
      "2025-11-06 20:53:06,681 - src.dataset.dataset_generator - INFO - ✓ train: 700 samples, 488.6 MB audio + 378.3 KB MIDI\n",
      "2025-11-06 20:53:35,712 - src.dataset.dataset_generator - INFO - ✓ val: 150 samples, 101.5 MB audio + 76.0 KB MIDI\n",
      "2025-11-06 20:53:35,712 - src.dataset.dataset_generator - INFO - ✓ val: 150 samples, 101.5 MB audio + 76.0 KB MIDI\n",
      "2025-11-06 20:54:04,740 - src.dataset.dataset_generator - INFO - ✓ test: 150 samples, 106.5 MB audio + 79.8 KB MIDI\n",
      "2025-11-06 20:54:04,757 - src.dataset.dataset_generator - INFO - Generated metadata files in data/datasets/training_run_1/training_run_1_v1.0.0_20251106_205052/metadata\n",
      "2025-11-06 20:54:04,757 - src.dataset.dataset_generator - INFO - Dataset generation complete: data/datasets/training_run_1/training_run_1_v1.0.0_20251106_205052\n",
      "\n",
      "✓ Dataset generated successfully!\n",
      "Location: data/datasets/training_run_1/training_run_1_v1.0.0_20251106_205052\n",
      "Total samples: 1000\n",
      "\n",
      "Splits:\n",
      "  Train: 700 samples\n",
      "  Val:   150 samples\n",
      "  Test:  150 samples\n",
      "\n",
      "Audio files: wav @ 16000Hz\n",
      "2025-11-06 20:54:04,740 - src.dataset.dataset_generator - INFO - ✓ test: 150 samples, 106.5 MB audio + 79.8 KB MIDI\n",
      "2025-11-06 20:54:04,757 - src.dataset.dataset_generator - INFO - Generated metadata files in data/datasets/training_run_1/training_run_1_v1.0.0_20251106_205052/metadata\n",
      "2025-11-06 20:54:04,757 - src.dataset.dataset_generator - INFO - Dataset generation complete: data/datasets/training_run_1/training_run_1_v1.0.0_20251106_205052\n",
      "\n",
      "✓ Dataset generated successfully!\n",
      "Location: data/datasets/training_run_1/training_run_1_v1.0.0_20251106_205052\n",
      "Total samples: 1000\n",
      "\n",
      "Splits:\n",
      "  Train: 700 samples\n",
      "  Val:   150 samples\n",
      "  Test:  150 samples\n",
      "\n",
      "Audio files: wav @ 16000Hz\n"
     ]
    }
   ],
   "source": [
    "# Generate dataset\n",
    "!python -m src.dataset.cli generate \\\n",
    "    --samples {SAMPLES} \\\n",
    "    --complexity-dist {COMPLEXITY_DIST} \\\n",
    "    --name {EXPERIMENT_NAME} \\\n",
    "    --output-dir data/datasets/{EXPERIMENT_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3267f6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found dataset: data/datasets/training_run_1/training_run_1_v1.0.0_20251106_205052\n",
      "Dataset: training_run_1 v1.0.0\n",
      "Generated: 2025-11-06T20:54:04.740127\n",
      "Location: data/datasets/training_run_1/training_run_1_v1.0.0_20251106_205052\n",
      "\n",
      "Total samples: 1000\n",
      "\n",
      "Splits:\n",
      "  train:  700 samples\n",
      "  val  :  150 samples\n",
      "  test :  150 samples\n",
      "\n",
      "Configuration:\n",
      "  Sample rate: 16000Hz\n",
      "  Audio format: wav\n",
      "\n",
      "Statistics:\n",
      "  Complexity:\n",
      "    beginner    : 266 ( 26.6%)\n",
      "    intermediate: 535 ( 53.5%)\n",
      "    advanced    : 199 ( 19.9%)\n",
      "  Tempo: 60-180 BPM (avg: 120.0)\n",
      "  Measures: 4-16 (avg: 9.9)\n",
      "  Duration: 380.4 min total (avg: 22.8s per sample)\n",
      "Dataset: training_run_1 v1.0.0\n",
      "Generated: 2025-11-06T20:54:04.740127\n",
      "Location: data/datasets/training_run_1/training_run_1_v1.0.0_20251106_205052\n",
      "\n",
      "Total samples: 1000\n",
      "\n",
      "Splits:\n",
      "  train:  700 samples\n",
      "  val  :  150 samples\n",
      "  test :  150 samples\n",
      "\n",
      "Configuration:\n",
      "  Sample rate: 16000Hz\n",
      "  Audio format: wav\n",
      "\n",
      "Statistics:\n",
      "  Complexity:\n",
      "    beginner    : 266 ( 26.6%)\n",
      "    intermediate: 535 ( 53.5%)\n",
      "    advanced    : 199 ( 19.9%)\n",
      "  Tempo: 60-180 BPM (avg: 120.0)\n",
      "  Measures: 4-16 (avg: 9.9)\n",
      "  Duration: 380.4 min total (avg: 22.8s per sample)\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset (find the versioned directory)\n",
    "import os\n",
    "dataset_base = f'data/datasets/{EXPERIMENT_NAME}'\n",
    "versioned_dirs = [d for d in os.listdir(dataset_base) if d.startswith(EXPERIMENT_NAME)]\n",
    "if versioned_dirs:\n",
    "    dataset_path = os.path.join(dataset_base, versioned_dirs[0])\n",
    "    print(f\"Found dataset: {dataset_path}\")\n",
    "    !python -m src.dataset.cli info {dataset_path}\n",
    "else:\n",
    "    print(f\"No dataset found in {dataset_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f4168f",
   "metadata": {},
   "source": [
    "## Step 3: Configure Model\n",
    "\n",
    "Set up model architecture and training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd49cc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration:\n",
      "  Hidden size: 256\n",
      "  Decoder layers: 6\n",
      "  Attention heads: 8\n",
      "\n",
      "Training configuration:\n",
      "  Learning rate: 0.0001\n",
      "  Batch size: 16\n",
      "  Epochs: 50\n",
      "  Mixed precision: True\n",
      "  Gradient accumulation: 4\n",
      "\n",
      "Dataset path: data/datasets/training_run_1/training_run_1_v1.0.0_20251106_205052\n"
     ]
    }
   ],
   "source": [
    "from src.model.config import ModelConfig, TrainingConfig, DataConfig\n",
    "from pathlib import Path\n",
    "\n",
    "# Use dataset_path from previous step\n",
    "# dataset_path was already set in the verification step above\n",
    "\n",
    "# Model configuration\n",
    "model_config = ModelConfig(\n",
    "    vocab_size=128,\n",
    "    hidden_size=256,\n",
    "    num_decoder_layers=6,\n",
    "    num_attention_heads=8,\n",
    "    dropout=0.1,\n",
    "    max_sequence_length=512,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Training configuration\n",
    "training_config = TrainingConfig(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=EPOCHS,\n",
    "    optimizer='adamw',\n",
    "    scheduler='cosine',\n",
    "    use_mixed_precision=True,\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_accumulation_steps=4,\n",
    "    checkpoint_dir='checkpoints',\n",
    "    save_every_n_epochs=0,  # Only save best and final\n",
    "    early_stopping_patience=10\n",
    ")\n",
    "\n",
    "# Data configuration\n",
    "data_config = DataConfig(\n",
    "    sample_rate=16000,\n",
    "    n_mels=128,\n",
    "    dataset_dir=Path(dataset_path)\n",
    ")\n",
    "\n",
    "print(\"Model configuration:\")\n",
    "print(f\"  Hidden size: {model_config.hidden_size}\")\n",
    "print(f\"  Decoder layers: {model_config.num_decoder_layers}\")\n",
    "print(f\"  Attention heads: {model_config.num_attention_heads}\")\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Learning rate: {training_config.learning_rate}\")\n",
    "print(f\"  Batch size: {training_config.batch_size}\")\n",
    "print(f\"  Epochs: {training_config.num_epochs}\")\n",
    "print(f\"  Mixed precision: {training_config.use_mixed_precision}\")\n",
    "print(f\"  Gradient accumulation: {training_config.gradient_accumulation_steps}\")\n",
    "print(f\"\\nDataset path: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc58bc",
   "metadata": {},
   "source": [
    "## Step 4: Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eca29fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits:\n",
      "  Train samples: 700\n",
      "  Val samples: 150\n",
      "  Test samples: 150\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    data_config, model_config, training_config\n",
    ")\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "print(f\"  Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"  Val samples: {len(val_loader.dataset)}\")\n",
    "print(f\"  Test samples: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef1d7c9",
   "metadata": {},
   "source": [
    "## Step 5: Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f4a9a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\n",
      "  Total: 95,922,304\n",
      "  Trainable: 9,735,040\n",
      "  Frozen: 86,187,264\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = Sound2SheetModel(model_config, freeze_encoder=True)\n",
    "\n",
    "# Count parameters\n",
    "params = model.count_parameters()\n",
    "print(f\"Model parameters:\")\n",
    "print(f\"  Total: {params['total']:,}\")\n",
    "print(f\"  Trainable: {params['trainable']:,}\")\n",
    "print(f\"  Frozen: {params['frozen']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bca7cf",
   "metadata": {},
   "source": [
    "## Step 6: Train Model\n",
    "\n",
    "Start training with progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7557ea56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volodymyr/VS code/My_projecs/Pythons/Sound2Sheet/src/model/trainer.py:97: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler() if training_config.use_mixed_precision else None\n",
      "/home/volodymyr/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:31: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/44 [00:00<?, ?it/s]/home/volodymyr/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/volodymyr/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/volodymyr/VS code/My_projecs/Pythons/Sound2Sheet/src/model/trainer.py:248: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/volodymyr/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/home/volodymyr/VS code/My_projecs/Pythons/Sound2Sheet/src/model/trainer.py:248: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/volodymyr/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Epoch 1:  16%|█▌        | 7/44 [02:40<14:09, 22.96s/it, loss=4.78]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining complete\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/src/model/trainer.py:170\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28mself\u001b[39m.current_epoch = epoch\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# Training epoch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m train_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28mself\u001b[39m.train_losses.append(train_loss)\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# Validation epoch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/src/model/trainer.py:249\u001b[39m, in \u001b[36mTrainer._train_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_mixed_precision:\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnotes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Teacher forcing (exclude last token)\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m         \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[32m    255\u001b[39m         \u001b[38;5;66;03m# outputs is logits tensor [batch, seq_len, vocab_size]\u001b[39;00m\n\u001b[32m    256\u001b[39m         logits = outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/src/model/sound2sheet_model.py:66\u001b[39m, in \u001b[36mSound2SheetModel.forward\u001b[39m\u001b[34m(self, mel, target_notes)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03mForward pass through model.\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     63\u001b[39m \u001b[33;03m    logits: Note predictions [batch, max_notes, vocab_size]\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Encode audio\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m encoder_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, enc_seq_len, hidden_size]\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Decode notes (decoder expects: encoder_hidden_states, target_notes)\u001b[39;00m\n\u001b[32m     69\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.decoder(encoder_output, target_notes)  \u001b[38;5;66;03m# [batch, max_notes, vocab_size]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/src/model/ast_model.py:90\u001b[39m, in \u001b[36mASTWrapper.forward\u001b[39m\u001b[34m(self, mel_spectrogram, attention_mask)\u001b[39m\n\u001b[32m     87\u001b[39m         attention_mask = attention_mask[:, :max_length]\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Forward through AST\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_spectrogram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Get sequence output (not pooled output)\u001b[39;00m\n\u001b[32m     93\u001b[39m hidden_states = outputs.last_hidden_state  \u001b[38;5;66;03m# [batch, seq_len, hidden_size]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:400\u001b[39m, in \u001b[36mASTModel.forward\u001b[39m\u001b[34m(self, input_values, head_mask, **kwargs)\u001b[39m\n\u001b[32m    396\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m    398\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(input_values)\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m encoder_outputs: BaseModelOutput = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m sequence_output = encoder_outputs.last_hidden_state\n\u001b[32m    402\u001b[39m sequence_output = \u001b[38;5;28mself\u001b[39m.layernorm(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:305\u001b[39m, in \u001b[36mASTEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layer):\n\u001b[32m    304\u001b[39m     layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m     hidden_states = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutput(last_hidden_state=hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:289\u001b[39m, in \u001b[36mASTLayer.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask)\u001b[39m\n\u001b[32m    286\u001b[39m layer_output = \u001b[38;5;28mself\u001b[39m.intermediate(layer_output)\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# second residual connection is done here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:257\u001b[39m, in \u001b[36mASTOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    259\u001b[39m     hidden_states = hidden_states + input_tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VS code/My_projecs/Pythons/Sound2Sheet/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_config=model_config,\n",
    "    training_config=training_config\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b8e5f",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Load training history\n",
    "history_file = Path('checkpoints/training_history.json')\n",
    "if history_file.exists():\n",
    "    with open(history_file, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    # Plot training curves\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history['val_accuracy'], label='Val Accuracy', color='green', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('Validation Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print best metrics\n",
    "    best_epoch = history['val_loss'].index(min(history['val_loss']))\n",
    "    print(f\"Best results:\")\n",
    "    print(f\"  Epoch: {best_epoch + 1}\")\n",
    "    print(f\"  Val Loss: {history['val_loss'][best_epoch]:.4f}\")\n",
    "    print(f\"  Val Accuracy: {history['val_accuracy'][best_epoch]:.4f}\")\n",
    "else:\n",
    "    print(\"Training history not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f08fa",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate Model\n",
    "\n",
    "Run evaluation on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe9f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import EvaluationConfig\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load('checkpoints/best_model.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Evaluation configuration\n",
    "eval_config = EvaluationConfig(\n",
    "    onset_tolerance=0.05,\n",
    "    offset_tolerance=0.05\n",
    ")\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = Evaluator(model, model_config, eval_config)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating on test set...\")\n",
    "metrics = evaluator.evaluate(test_loader)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nEvaluation results:\")\n",
    "print(f\"  Note Accuracy: {metrics['note_accuracy']:.2%}\")\n",
    "print(f\"  Onset F1: {metrics['onset_f1']:.2%}\")\n",
    "print(f\"  Offset F1: {metrics['offset_f1']:.2%}\")\n",
    "print(f\"  Pitch Precision: {metrics['pitch_precision']:.2%}\")\n",
    "print(f\"  Pitch Recall: {metrics['pitch_recall']:.2%}\")\n",
    "print(f\"  Timing Deviation: {metrics['timing_deviation_ms']:.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87b578e",
   "metadata": {},
   "source": [
    "## Step 9: Test Inference\n",
    "\n",
    "Transcribe a sample from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ecd6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.config import InferenceConfig\n",
    "from src.converter import NoteBuilder\n",
    "\n",
    "# Get a test sample\n",
    "test_sample = test_loader.dataset[0]\n",
    "mel_spec = test_sample['mel_spec'].unsqueeze(0).to(device)\n",
    "\n",
    "# Inference configuration\n",
    "inference_config = InferenceConfig(\n",
    "    strategy='greedy',\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Transcribing sample...\")\n",
    "with torch.no_grad():\n",
    "    predictions = model.generate(mel_spec, inference_config)\n",
    "\n",
    "print(f\"Predicted {len(predictions)} notes\")\n",
    "print(f\"Notes: {predictions[:20]}...\")\n",
    "\n",
    "# Convert to MIDI\n",
    "note_builder = NoteBuilder()\n",
    "note_sequence = note_builder.build_from_predictions(\n",
    "    predictions,\n",
    "    quantize=True,\n",
    "    quantization_resolution=16\n",
    ")\n",
    "\n",
    "# Save MIDI\n",
    "note_sequence.to_midi('transcribed_sample.mid')\n",
    "print(\"\\nSaved: transcribed_sample.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f90e1e",
   "metadata": {},
   "source": [
    "## Step 10: Save Final Model\n",
    "\n",
    "Save the trained model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is already saved in checkpoints/\n",
    "# best_model.pt - best model by validation loss\n",
    "# final_model.pt - final model after all epochs\n",
    "\n",
    "print(\"Model files:\")\n",
    "print(\"  checkpoints/best_model.pt\")\n",
    "print(\"  checkpoints/final_model.pt\")\n",
    "print(\"  checkpoints/training_history.json\")\n",
    "print(\"\\nDownload these files to use the model locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4850209b",
   "metadata": {},
   "source": [
    "## Optional: Resume Training\n",
    "\n",
    "If training was interrupted, resume from checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c69efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training example (uncomment to use)\n",
    "\"\"\"\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load('checkpoints/best_model.pt', map_location=device)\n",
    "\n",
    "# Create new trainer with loaded model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_config=model_config,\n",
    "    training_config=training_config,\n",
    "    resume_from='checkpoints/best_model.pt'\n",
    ")\n",
    "\n",
    "# Continue training\n",
    "trainer.train()\n",
    "\"\"\"\n",
    "print(\"Resume training code available (commented out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b105b98",
   "metadata": {},
   "source": [
    "## Optional: Custom Configuration\n",
    "\n",
    "Adjust hyperparameters for different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Larger model\n",
    "\"\"\"\n",
    "model_config = ModelConfig(\n",
    "    hidden_size=512,\n",
    "    num_decoder_layers=8,\n",
    "    num_attention_heads=16,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Example: More aggressive training\n",
    "training_config = TrainingConfig(\n",
    "    learning_rate=5e-5,\n",
    "    batch_size=8,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_epochs=100\n",
    ")\n",
    "\"\"\"\n",
    "print(\"Custom configuration examples available (commented out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e269a767",
   "metadata": {},
   "source": [
    "## Training Complete\n",
    "\n",
    "Model is trained and ready for use. Download checkpoint files to use locally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

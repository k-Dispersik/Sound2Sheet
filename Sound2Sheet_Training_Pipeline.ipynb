{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c64483",
   "metadata": {},
   "source": [
    "# Sound2Sheet Training Pipeline\n",
    "\n",
    "Complete training pipeline for audio-to-sheet-music transcription model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a30caa",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7593ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2549346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/k-Dispersik/Sound2Sheet.git\n",
    "%cd Sound2Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install system dependencies\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq fluidsynth fluid-soundfont-gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python packages\n",
    "!pip install -q torch torchaudio transformers librosa numpy pandas tqdm pyyaml \\\n",
    "    pretty_midi music21 matplotlib mido midi2audio accelerate scipy scikit-learn\n",
    "\n",
    "print(\"Installation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8cc902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify imports\n",
    "from src.dataset import DatasetGenerator, MIDIGenerator\n",
    "from src.core import AudioProcessor\n",
    "from src.model import Sound2SheetModel, Trainer, create_dataloaders\n",
    "from src.converter import NoteBuilder\n",
    "from src.evaluation import Evaluator\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a6f019",
   "metadata": {},
   "source": [
    "## Step 2: Generate Dataset\n",
    "\n",
    "Configure and generate synthetic training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "SAMPLES = 1000          # Total samples (train + val + test)\n",
    "COMPLEXITY = \"medium\"   # simple, medium, complex\n",
    "EPOCHS = 50             # Training epochs\n",
    "BATCH_SIZE = 16         # Batch size\n",
    "LEARNING_RATE = 1e-4    # Learning rate\n",
    "EXPERIMENT_NAME = \"training_run_1\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Samples: {SAMPLES}\")\n",
    "print(f\"  Complexity: {COMPLEXITY}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9eb523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "!python -m src.dataset.cli generate \\\n",
    "    --samples {SAMPLES} \\\n",
    "    --complexity {COMPLEXITY} \\\n",
    "    --name {EXPERIMENT_NAME} \\\n",
    "    --output-dir data/datasets/{EXPERIMENT_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3267f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset\n",
    "!python -m src.dataset.cli info \\\n",
    "    --dataset-dir data/datasets/{EXPERIMENT_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f4168f",
   "metadata": {},
   "source": [
    "## Step 3: Configure Model\n",
    "\n",
    "Set up model architecture and training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd49cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.config import ModelConfig, TrainingConfig, DataConfig\n",
    "from pathlib import Path\n",
    "\n",
    "# Model configuration\n",
    "model_config = ModelConfig(\n",
    "    vocab_size=128,\n",
    "    hidden_size=256,\n",
    "    num_decoder_layers=6,\n",
    "    num_attention_heads=8,\n",
    "    dropout=0.1,\n",
    "    max_sequence_length=512,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Training configuration\n",
    "training_config = TrainingConfig(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=EPOCHS,\n",
    "    optimizer='adamw',\n",
    "    scheduler='cosine',\n",
    "    use_amp=True,\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_accumulation_steps=4,\n",
    "    checkpoint_dir='checkpoints',\n",
    "    save_every_n_epochs=0,  # Only save best and final\n",
    "    early_stopping_patience=10\n",
    ")\n",
    "\n",
    "# Data configuration\n",
    "data_config = DataConfig(\n",
    "    sample_rate=16000,\n",
    "    n_mels=128,\n",
    "    max_audio_length=10.0,\n",
    "    max_notes=512,\n",
    "    manifest_dir=Path(f'data/datasets/{EXPERIMENT_NAME}')\n",
    ")\n",
    "\n",
    "print(\"Model configuration:\")\n",
    "print(f\"  Hidden size: {model_config.hidden_size}\")\n",
    "print(f\"  Decoder layers: {model_config.num_decoder_layers}\")\n",
    "print(f\"  Attention heads: {model_config.num_attention_heads}\")\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Learning rate: {training_config.learning_rate}\")\n",
    "print(f\"  Batch size: {training_config.batch_size}\")\n",
    "print(f\"  Epochs: {training_config.num_epochs}\")\n",
    "print(f\"  Mixed precision: {training_config.use_amp}\")\n",
    "print(f\"  Gradient accumulation: {training_config.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc58bc",
   "metadata": {},
   "source": [
    "## Step 4: Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca29fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    data_config, model_config, training_config\n",
    ")\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "print(f\"  Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"  Val samples: {len(val_loader.dataset)}\")\n",
    "print(f\"  Test samples: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef1d7c9",
   "metadata": {},
   "source": [
    "## Step 5: Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4a9a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Sound2SheetModel(model_config, freeze_encoder=True)\n",
    "\n",
    "# Count parameters\n",
    "params = model.count_parameters()\n",
    "print(f\"Model parameters:\")\n",
    "print(f\"  Total: {params['total']:,}\")\n",
    "print(f\"  Trainable: {params['trainable']:,}\")\n",
    "print(f\"  Frozen: {params['frozen']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bca7cf",
   "metadata": {},
   "source": [
    "## Step 6: Train Model\n",
    "\n",
    "Start training with progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7557ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_config=model_config,\n",
    "    training_config=training_config\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b8e5f",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Load training history\n",
    "history_file = Path('checkpoints/training_history.json')\n",
    "if history_file.exists():\n",
    "    with open(history_file, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    # Plot training curves\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history['val_accuracy'], label='Val Accuracy', color='green', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('Validation Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print best metrics\n",
    "    best_epoch = history['val_loss'].index(min(history['val_loss']))\n",
    "    print(f\"Best results:\")\n",
    "    print(f\"  Epoch: {best_epoch + 1}\")\n",
    "    print(f\"  Val Loss: {history['val_loss'][best_epoch]:.4f}\")\n",
    "    print(f\"  Val Accuracy: {history['val_accuracy'][best_epoch]:.4f}\")\n",
    "else:\n",
    "    print(\"Training history not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f08fa",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate Model\n",
    "\n",
    "Run evaluation on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe9f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import EvaluationConfig\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load('checkpoints/best_model.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Evaluation configuration\n",
    "eval_config = EvaluationConfig(\n",
    "    onset_tolerance=0.05,\n",
    "    offset_tolerance=0.05\n",
    ")\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = Evaluator(model, model_config, eval_config)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating on test set...\")\n",
    "metrics = evaluator.evaluate(test_loader)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nEvaluation results:\")\n",
    "print(f\"  Note Accuracy: {metrics['note_accuracy']:.2%}\")\n",
    "print(f\"  Onset F1: {metrics['onset_f1']:.2%}\")\n",
    "print(f\"  Offset F1: {metrics['offset_f1']:.2%}\")\n",
    "print(f\"  Pitch Precision: {metrics['pitch_precision']:.2%}\")\n",
    "print(f\"  Pitch Recall: {metrics['pitch_recall']:.2%}\")\n",
    "print(f\"  Timing Deviation: {metrics['timing_deviation_ms']:.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87b578e",
   "metadata": {},
   "source": [
    "## Step 9: Test Inference\n",
    "\n",
    "Transcribe a sample from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ecd6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.config import InferenceConfig\n",
    "from src.converter import NoteBuilder\n",
    "\n",
    "# Get a test sample\n",
    "test_sample = test_loader.dataset[0]\n",
    "mel_spec = test_sample['mel_spec'].unsqueeze(0).to(device)\n",
    "\n",
    "# Inference configuration\n",
    "inference_config = InferenceConfig(\n",
    "    strategy='greedy',\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Transcribing sample...\")\n",
    "with torch.no_grad():\n",
    "    predictions = model.generate(mel_spec, inference_config)\n",
    "\n",
    "print(f\"Predicted {len(predictions)} notes\")\n",
    "print(f\"Notes: {predictions[:20]}...\")\n",
    "\n",
    "# Convert to MIDI\n",
    "note_builder = NoteBuilder()\n",
    "note_sequence = note_builder.build_from_predictions(\n",
    "    predictions,\n",
    "    quantize=True,\n",
    "    quantization_resolution=16\n",
    ")\n",
    "\n",
    "# Save MIDI\n",
    "note_sequence.to_midi('transcribed_sample.mid')\n",
    "print(\"\\nSaved: transcribed_sample.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f90e1e",
   "metadata": {},
   "source": [
    "## Step 10: Save Final Model\n",
    "\n",
    "Save the trained model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is already saved in checkpoints/\n",
    "# best_model.pt - best model by validation loss\n",
    "# final_model.pt - final model after all epochs\n",
    "\n",
    "print(\"Model files:\")\n",
    "print(\"  checkpoints/best_model.pt\")\n",
    "print(\"  checkpoints/final_model.pt\")\n",
    "print(\"  checkpoints/training_history.json\")\n",
    "print(\"\\nDownload these files to use the model locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4850209b",
   "metadata": {},
   "source": [
    "## Optional: Resume Training\n",
    "\n",
    "If training was interrupted, resume from checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c69efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training example (uncomment to use)\n",
    "\"\"\"\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load('checkpoints/best_model.pt', map_location=device)\n",
    "\n",
    "# Create new trainer with loaded model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_config=model_config,\n",
    "    training_config=training_config,\n",
    "    resume_from='checkpoints/best_model.pt'\n",
    ")\n",
    "\n",
    "# Continue training\n",
    "trainer.train()\n",
    "\"\"\"\n",
    "print(\"Resume training code available (commented out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b105b98",
   "metadata": {},
   "source": [
    "## Optional: Custom Configuration\n",
    "\n",
    "Adjust hyperparameters for different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Larger model\n",
    "\"\"\"\n",
    "model_config = ModelConfig(\n",
    "    hidden_size=512,\n",
    "    num_decoder_layers=8,\n",
    "    num_attention_heads=16,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Example: More aggressive training\n",
    "training_config = TrainingConfig(\n",
    "    learning_rate=5e-5,\n",
    "    batch_size=8,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_epochs=100\n",
    ")\n",
    "\"\"\"\n",
    "print(\"Custom configuration examples available (commented out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e269a767",
   "metadata": {},
   "source": [
    "## Training Complete\n",
    "\n",
    "Model is trained and ready for use. Download checkpoint files to use locally."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
